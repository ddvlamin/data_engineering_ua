{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labsession Pyspark\n",
    "In this labsession we will be covering pyspark, the pythong implementation of Apache Spark, which uses the MapReduce paradigm. As was the case for previous sessions, fill in the CODE_HERE placeholders.\n",
    "\n",
    "The documentation for Pyspark is located at https://spark.apache.org/docs/latest/api/python/pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "#uncomment this when you installed pyspark yourself on a Windows computer following the given tutorial.\n",
    "#import findspark\n",
    "#findspark.init()\n",
    "import numpy as np\n",
    "import pyspark\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "conf = pyspark.SparkConf().setAppName('appName').setMaster('local')\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First  let's rehearse some basic python concepts that we need in this lab session\n",
    "\n",
    "### play around with the code to see how it behaves, use print to see what it's doing!\n",
    "\n",
    "#### The lambda function and its equivalent normal definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "lambda_add_2 = lambda x: x+2\n",
    "\n",
    "def add_2(x):\n",
    "    return x+2\n",
    "\n",
    "print(lambda_add_2(10))\n",
    "print(add_2(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to access nested JSON keys and values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "js_obj[\"list\"] = [0, 1, 2, 3]\n",
      "js_obj[\"list\"][1] = 1\n",
      "js_obj[\"nested_dict\"][\"key\"] = value\n",
      "js_obj[\"nested_list_of_dicts\"][1][\"key\"] = 1\n"
     ]
    }
   ],
   "source": [
    "js_obj = {\n",
    "    \"list\": [0,1,2,3],\n",
    "    \"nested_dict\": {\n",
    "        \"key\": \"value\"\n",
    "    },\n",
    "    \"nested_list_of_dicts\": [\n",
    "        {\n",
    "            \"key\": 0\n",
    "        },\n",
    "        {\n",
    "            \"key\": 1\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "print()\n",
    "print(f\"\"\"js_obj[\"list\"] = {js_obj[\"list\"]}\"\"\")\n",
    "print(f\"\"\"js_obj[\"list\"][1] = {js_obj[\"list\"][1]}\"\"\")\n",
    "print(f\"\"\"js_obj[\"nested_dict\"][\"key\"] = {js_obj[\"nested_dict\"][\"key\"]}\"\"\")\n",
    "print(f\"\"\"js_obj[\"nested_list_of_dicts\"][1][\"key\"] = {js_obj[\"nested_list_of_dicts\"][1][\"key\"]}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing nested tuples (you can see tuples as immutable lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tpl[1] = ({'dict_key': 'dict_value'}, 1)\n",
      "tpl[1][0] = {'dict_key': 'dict_value'}\n",
      "tpl[1][0]['dict_key'] = dict_value\n"
     ]
    }
   ],
   "source": [
    "tpl = (\"key\",({\"dict_key\":\"dict_value\"}, 1))\n",
    "print(f\"tpl[1] = {tpl[1]}\")\n",
    "print(f\"tpl[1][0] = {tpl[1][0]}\")\n",
    "print(f\"tpl[1][0]['dict_key'] = {tpl[1][0]['dict_key']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterating over keys and values of a dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 1\n",
      "b 2\n",
      "c 3\n"
     ]
    }
   ],
   "source": [
    "js_obj = {\n",
    "    \"a\": 1,\n",
    "    \"b\": 2,\n",
    "    \"c\": 3\n",
    "}\n",
    "for key, value in js_obj.items():\n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List comprehensions and their equivalent in a regular for-loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2]\n",
      "[0, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "list1 = [i for i in range(3)]\n",
    "list2 = []\n",
    "for i in range(3):\n",
    "    list2.append(i)\n",
    "print(list1)\n",
    "print(list2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dict comprehensions and their equivalent in a regular for-loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 2, 'b': 4, 'c': 6}\n",
      "{'a': 2, 'b': 4, 'c': 6}\n"
     ]
    }
   ],
   "source": [
    "multiplied_dict1 = {k: v*2 for k, v in js_obj.items()}\n",
    "multiplied_dict2 = {}\n",
    "for k, v in js_obj.items():\n",
    "    multiplied_dict2[k] = v*2\n",
    "print(multiplied_dict1)\n",
    "print(multiplied_dict2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular expressions: first compiling into a regex object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('bla',)\n",
      "('bla',)\n",
      "(None,)\n"
     ]
    }
   ],
   "source": [
    "match_str = \"blabla\"\n",
    "nomatch_str = \"blbl\"\n",
    "bla_regex = re.compile(\"(bla)*\")\n",
    "print(bla_regex.search(match_str).groups())\n",
    "print(re.search(\"(bla)*\",match_str).groups())\n",
    "print(bla_regex.search(nomatch_str).groups())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding all regex matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bla', 'bla', 'bla']\n"
     ]
    }
   ],
   "source": [
    "print(re.findall(\"bla\",\"bla132465789bla#bla\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise 1: Wordcount\n",
    "For the first excercise we will be performing wordcount on a sample file.\n",
    "\n",
    "First, the textfile, located in \"data/wordcount.txt\" needs to be converted to an RDD, and then split up.\n",
    "Hint: use sc.textFile and flatMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a']\n"
     ]
    }
   ],
   "source": [
    "words = sc.textFile(\"data/wordcount.txt\").flatMap(lambda line: line.split(\" \"))\n",
    "print(words.take(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, count the actual word. Hint: Use map & reduceByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('This', 1)]\n"
     ]
    }
   ],
   "source": [
    "wordCounts=words.map(lambda word: (word, 1)).reduceByKey(lambda a,b:a +b)\n",
    "print(wordCounts.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, save the output in a human readable format. You can check the output of the wordcount by going to \"data/output/part-00000\". Note, if you re-run the following black, you wil get errors because the output files already exist. If you want to re-run this part of the code, delete the output files first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCounts.saveAsTextFile(\"data/output/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise 2: Ecommerce data\n",
    "In this integrated excercise, we will be processing ecommerce data\n",
    "### Let's load the ecommerce data and transform it into an RDD\n",
    "#### the file contains JSON objects of the form\n",
    "\n",
    "```\n",
    "{\n",
    "  \"\": \"9\",\n",
    "  \"Clothing ID\": \"1077\",\n",
    "  \"Age\": \"34\",\n",
    "  \"Title\": \"Such a fun dress!\",\n",
    "  \"Review Text\": \"I'm 5\\\"5' and 125 lbs. i ordered the s petite to make sure the length wasn't too long. i\n",
    " typically wear an xs regular in retailer dresses. if you're less busty (34b cup or smaller), a s petite w\n",
    "ill fit you perfectly (snug, but not tight). i love that i could dress it up for a party, or down for work\n",
    ". i love that the tulle is longer then the fabric underneath.\",\n",
    "  \"Rating\": \"5\",\n",
    "  \"Recommended IND\": \"1\",\n",
    "  \"Positive Feedback Count\": \"0\",\n",
    "  \"Division Name\": \"General\",\n",
    "  \"Department Name\": \"Dresses\",\n",
    "  \"Class Name\": \"Dresses\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['{\"\": \"0\", \"Clothing ID\": \"767\", \"Age\": \"33\", \"Title\": \"\", \"Review Text\": \"Absolutely wonderful - silky and sexy and comfortable\", \"Rating\": \"4\", \"Recommended IND\": \"1\", \"Positive Feedback Count\": \"0\", \"Division Name\": \"Initmates\", \"Department Name\": \"Intimate\", \"Class Name\": \"Intimates\"}']\n"
     ]
    }
   ],
   "source": [
    "ecommerce_data = sc.textFile(\"./data/ecommerce.json\")\n",
    "print(ecommerce_data.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First convert each line (which is a string) to JSON using json.loads\n",
    "\n",
    "#### remember: Spark uses lazy evaluation so applying map on an RDD does not apply the function on the data until you call a fucntion like collect(), take(), reduce(), count(), saveAsTextFile()... which triggers the execution\n",
    "\n",
    "#### Hint: also put the RDD in memory using persist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'': '0', 'Clothing ID': '767', 'Age': '33', 'Title': '', 'Review Text': 'Absolutely wonderful - silky and sexy and comfortable', 'Rating': '4', 'Recommended IND': '1', 'Positive Feedback Count': '0', 'Division Name': 'Initmates', 'Department Name': 'Intimate', 'Class Name': 'Intimates'}]\n"
     ]
    }
   ],
   "source": [
    "ecommerce_json = ecommerce_data.map(json.loads).persist()\n",
    "print(ecommerce_json.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's first try to determine the average age of the reviewers on this e-commerce website\n",
    "\n",
    "#### First construct an RDD where we extract the Age field of each JSON record and transform it to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33]\n"
     ]
    }
   ],
   "source": [
    "age_rdd = ecommerce_json.map(lambda js: int(js[\"Age\"]))\n",
    "print(age_rdd.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's make a sum of all ages so that we can later devide by the total number of records to get the average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1014561\n"
     ]
    }
   ],
   "source": [
    "age_sum = age_rdd.reduce(lambda x, y: x+y)\n",
    "print(age_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we still need to figure out how many records are in the RDD, use the count() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_records = age_rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### compute the average now using the sum and the number of records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_age = age_sum/n_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.isclose(avg_age,43.1985438)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will now do the same, but compute the average number of words in the Review Text\n",
    "#### you can simply use the split function with the space character to split the text into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nwords_sum = ecommerce_json.map(lambda js: len(js[\"Review Text\"].split(\" \"))).reduce(lambda x, y: x+y)\n",
    "avg_nwords = nwords_sum / n_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.isclose(avg_nwords, 58.0843907)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next we will count the number of reviews per rating (i.e. 1, 2, 3, 4 and 5)\n",
    "\n",
    "#### Similar to the word count exercise, build a RDD of tuples with the rating and a count of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_counts = ecommerce_json.map(lambda js: (js[\"Rating\"],1)).reduceByKey(lambda x, y: x+y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sort the resulting tuples (after collecting the results) from high to low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_rating_counts = sorted(rating_counts, key = lambda x: x[1], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sorted_rating_counts[0] == ('5', 13131)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A second way to achieve the same thing is by first grouping all records together with the same key and then counting how many appear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_counts = ecommerce_json.map(lambda js: (js[\"Rating\"], None)).groupByKey().map(lambda x: (x[0],len(x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_rating_counts = sorted(rating_counts.collect(), key = lambda x: x[1], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sorted_rating_counts[0] == ('5', 13131)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next we will count the number of reviews per Department Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_counts = ecommerce_json.map(lambda js: (js[\"Department Name\"],1)).reduceByKey(lambda x, y: x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### let's filter out all the Department names with less than 1000 reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_counts_filtered = category_counts.filter(lambda x: x[1]>=1000).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(category_counts_filtered) == 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### select the Department with most reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "largest_category = sorted(category_counts_filtered, key = lambda x: x[1], reverse=True)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert largest_category == \"Tops\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### slightly more complex, let's now count the number of reviews per Department Name and Rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_rating_counts = (ecommerce_json\n",
    "                          .map(lambda js: ((js[\"Department Name\"],js[\"Rating\"]),1))\n",
    "                          .reduceByKey(lambda x, y: x+y)\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### only keep the counts for the Jackets department and sort from high to low number of reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "jackets = category_rating_counts.filter(lambda x: x[0][0]==\"Jackets\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_jackets = sorted(jackets, key = lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sorted_jackets[0] == ((\"Jackets\",'5'), 631)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create per Clothing ID a list of all the ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2agelist = (ecommerce_json\n",
    "              .map(lambda js: (js[\"Clothing ID\"], int(js[\"Age\"])))\n",
    "              .groupByKey()\n",
    "              .map(lambda x: (x[0],list(x[1])))\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### keep only the Clothing IDs that have more than 500 reviews and compute both the average and standard deviation of the age per Clothing ID\n",
    "\n",
    "#### The output should be tuples of the following form\n",
    "\n",
    "```\n",
    "('Clothing ID', {\"avg\": 43.4564, \"std\": 12.14566})\n",
    "```\n",
    "\n",
    "Hint: use np.array and np.std and np.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_summary_statistics(tpl):\n",
    "    id, age_list = tpl\n",
    "    age_array = np.array(age_list)\n",
    "    return (id, {\"avg\": np.mean(age_array), \"std\": np.std(age_array)})\n",
    "\n",
    "stats = id2agelist.filter(lambda x: len(x[1])>500).map(compute_summary_statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sort the results according the average age from high to low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_stats = sorted(stats.collect(), key=lambda x:x[1][\"avg\"], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sorted_stats[0] == ('829', {'avg': 44.64136622390892, 'std': 12.447020037376479})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next we will compute the term frequency - inverse document frequency of all the words in the reviews\n",
    "\n",
    "https://en.wikipedia.org/wiki/Tf%E2%80%93idf <br>\n",
    "\n",
    "#### First we will compute the document frequencies, i.e. count how many times each word occurs in a review (use a regular expression for this to filter out real words (no punctuation and numbers)\n",
    "\n",
    "The output of the document_frequencies RDD should be tuples of the form\n",
    "\n",
    "```\n",
    "('word', document_count)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the regular expression to extract only words with at least one character\n",
    "regex = re.compile(\"[a-zA-Z]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_terms(js):\n",
    "    words = set(regex.findall(js[\"Review Text\"]))\n",
    "    for word in words:\n",
    "        yield (word.lower(),1)\n",
    "      \n",
    "#use flatMap\n",
    "document_frequencies = ecommerce_json.flatMap(document_terms).reduceByKey(lambda x, y: x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert document_frequencies.filter(lambda x: x[0]==\"wonderful\").collect()[0][1] == 290"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we will compute the inverse document frequency using the formula np.log(number of documents/document count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('absolutely', 3.3820511804752176), ('sexy', 4.717052247207557)]\n"
     ]
    }
   ],
   "source": [
    "inverse_document_frequencies = document_frequencies.map(lambda x: (x[0], np.log(n_records/x[1])))\n",
    "print(inverse_document_frequencies.take(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sorted(inverse_document_frequencies.collect(), key = lambda x: x[1], reverse=True)[0][0] == \"narrowing\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will now compute the term frequencies per document\n",
    "\n",
    "#### The output should be an RDD of tuples of the form\n",
    "\n",
    "The document_id can be found under the empty key, i.e. js[\"\"]\n",
    "\n",
    "```\n",
    "('word', ('document_id', count))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('absolutely', ('0', 1)), ('wonderful', ('0', 1))]\n"
     ]
    }
   ],
   "source": [
    "def document_term_frequencies(js):\n",
    "    record_id = js[\"\"]\n",
    "    words = regex.findall(js[\"Review Text\"])\n",
    "    word_count = Counter()\n",
    "    for word in words:\n",
    "        word_count[(record_id, word.lower())] += 1\n",
    "    return [(word,(record_id,cnt)) for ((record_id, word), cnt) in word_count.items()]\n",
    "\n",
    "term_frequencies = ecommerce_json.flatMap(document_term_frequencies)\n",
    "print(term_frequencies.take(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert term_frequencies.filter(lambda x: x[0]==\"it\" and x[1][0]=='1').collect()[0][1][1] == 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we need to join the document frequencies with the term frequencies per document\n",
    "\n",
    "#### the output of the join will be an RDD with tuples of the form\n",
    "\n",
    "```\n",
    "('word', (idf, ('document_id', word count/term frequency) ) )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('comfortable', (2.073244314833701, ('0', 1))), ('comfortable', (2.073244314833701, ('2', 1)))]\n"
     ]
    }
   ],
   "source": [
    "tf_idf_joined = inverse_document_frequencies.join(term_frequencies)\n",
    "print(tf_idf_joined.take(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tf_idf_joined.filter(lambda x: x[0]=='comfortable' and x[1][1][0]==\"0\").collect()[0] == ('comfortable', (2.073244314833701, ('0', 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now lets multiply the term frequency/word count of each document with the inverse document frequency of the word\n",
    "\n",
    "#### and now make the keys (i.e. first element of the tuples) of the records in the resulting RDD the id of the document\n",
    "\n",
    "The output of this RDD should look like:\n",
    "\n",
    "```\n",
    "('document_id', ('word', tf*idf))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('0', ('comfortable', 2.073244314833701)), ('2', ('comfortable', 2.073244314833701))]\n"
     ]
    }
   ],
   "source": [
    "tf_idf = tf_idf_joined.map(lambda x: (x[1][1][0], (x[0], x[1][0] * x[1][1][1])))\n",
    "print(tf_idf.take(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.isclose(tf_idf.filter(lambda x: x[0]=='0' and x[1][0]==\"comfortable\").collect()[0][1][1], 2.07324431)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally we need to group all words that belong to the same document\n",
    "\n",
    "#### The output should be a tuple of the form ('document_id', words_dict) where words_dict is a dict of the following format\n",
    "\n",
    "```\n",
    "{\n",
    "    \"word\": tf_idf_word\n",
    "    \"word2\": tf_idf_word2\n",
    "    ...\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_tf_idf(tpl):\n",
    "    id, tpls = tpl\n",
    "    words = {word: tf_idf for word, tf_idf in tpls}\n",
    "    return (id, words)\n",
    "        \n",
    "per_review = tf_idf.groupByKey().map(review_tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected = {'comfortable': 2.073244314833701, 'silky': 5.235846040622725, 'wonderful': 4.394278854944507, 'absolutely': 3.3820511804752176, 'sexy': 4.717052247207557, 'and': 0.36522226581530454}\n",
    "assert per_review.filter(lambda x: x[0] == '0').collect()[0][1] == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
