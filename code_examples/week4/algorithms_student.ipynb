{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this lab session we will implement two algorithms (an algorithm for sorting and one for detecting communities) each in two different ways: a naive, less efficient way and a better, faster way\n",
    "\n",
    "### Fill in the CODE_HERE placeholders and check your code by evaluating the boxes with assert statements\n",
    "\n",
    "### Make sure to evaluate every box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let us first install the necessary libraries\n",
    "!pip install matplotlib\n",
    "!pip install plotly\n",
    "!pip install networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let us import the necessary libraries\n",
    "from collections import defaultdict\n",
    "from itertools import count\n",
    "\n",
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.offline as py\n",
    "from plotly.offline import init_notebook_mode\n",
    "import plotly.graph_objs as go\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's start with the sorting algorithm\n",
    "We will create multiple data sets of integers we want to sort <br>\n",
    "Each data set bigger than the previous one <br>\n",
    "These different sized data sets will be used to see how computation time increases with the increasing size of the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_and_time_sorting(datasets, sort_function):\n",
    "    datasets = copy.deepcopy(datasets)\n",
    "    timings = []\n",
    "    for dataset in datasets:\n",
    "        start = time.time()\n",
    "        sort_function(dataset)\n",
    "        end = time.time()\n",
    "        time_difference = end - start\n",
    "        timings.append(time_difference)\n",
    "    return timings\n",
    "\n",
    "data_sizes = [5000,10000,20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = []\n",
    "for data_size in data_sizes:\n",
    "    datasets.append(np.random.permutation(data_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we will implement the naive insertion sorting algorithm \n",
    "It's an algorithm that sorts an array in-place by iterating over the elements and inserting the current element\n",
    "in the part of the array that is already sorted\n",
    "\n",
    "```python\n",
    "[1,11,13,7,4,8,0]\n",
    "```\n",
    "\n",
    "If the current element is 7, then the part of the algorithm before 7 is already sorted by the algorithm <br>\n",
    "It then tries to find the position to insert the element 7 in the part of the array that is already sorted <br>\n",
    "Resulting in\n",
    "\n",
    "```python\n",
    "[1,7,11,13,4,8,0]\n",
    "```\n",
    "\n",
    "The next element to be sorted is 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define naive sort\n",
    "def naivesort(data):\n",
    "    \"\"\"\n",
    "    sorts the data in place\n",
    "    \"\"\"\n",
    "    for i, element in enumerate(data):\n",
    "        j = i\n",
    "        #iterate back into the array as long as the current element is smaller than the previous one\n",
    "        while j>0 and CODE_HERE: \n",
    "            data[j] = CODE_HERE #put the previous larger element in the position of the current one\n",
    "            j -= 1\n",
    "        data[j] = CODE_HERE # store the current element in its sorted position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform naivesort on data\n",
    "data = np.random.permutation(10)\n",
    "naivesort(data)\n",
    "assert np.all(data == np.array([0,1,2,3,4,5,6,7,8,9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform naivesort on datasets with different sizes and store timings\n",
    "timings_slow = apply_and_time_sorting(datasets, naivesort)\n",
    "print(timings_slow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's compare this with the quicksort algorithm that was discussed in class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = []\n",
    "for data_size in data_sizes:\n",
    "    datasets.append(np.random.permutation(data_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions\n",
    "def swap(l, i, j):\n",
    "    \"\"\"\n",
    "    swaps element in position i to position j and vice versa in the list l\n",
    "    \"\"\"\n",
    "    tmp = l[j]\n",
    "    l[j] = l[i]\n",
    "    l[i] = tmp\n",
    "\n",
    "def quicksort_helper(l, starti, endi):\n",
    "    #set_trace()\n",
    "    if starti >= endi:\n",
    "        return\n",
    "    #if the data set contains less than 100 elements, using the naive insertion sort is better\n",
    "    if endi-starti+1<=100:\n",
    "        naivesort(l)\n",
    "    else:\n",
    "        middlei = int(0.5*(starti+endi))\n",
    "        #first make sure the start, middle and end element are in sorted order\n",
    "        if (l[middlei]<l[starti]):\n",
    "            swap(l,CODE_HERE, CODE_HERE)\n",
    "        if (l[endi]<l[starti]):\n",
    "            swap(l, CODE_HERE, CODE_HERE)\n",
    "        if (l[endi]<l[middlei]):\n",
    "            swap(l, CODE_HERE, CODE_HERE)\n",
    "        pivoti = middlei\n",
    "        \n",
    "        #put the pivot element just before the last \n",
    "        swap(l, CODE_HERE, CODE_HERE)\n",
    "        pivot = l[endi-1]\n",
    "        lefti = starti+1\n",
    "        righti = endi-2\n",
    "        \n",
    "        #now make sure the elements are partially sorted,\n",
    "        #i.e. elements in the first half should be smaller than the pivot element\n",
    "        #the elements in the second half should be larger than the pivot element\n",
    "        while True:\n",
    "            #move the lefti cursor from left to right until you encounter an element larger than the pivot\n",
    "            while l[lefti]<pivot: \n",
    "                CODE_HERE\n",
    "            #move the righti cursor from right to left until you encounter an element smaller than the pivot\n",
    "            while pivot<l[righti]: \n",
    "                CODE_HERE\n",
    "            #if the lefti cursor is still lower than the righi cursor, swap them, otherwise break out of the loop\n",
    "            if lefti < righti:  \n",
    "                swap(l, CODE_HERE, CODE_HERE)\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "        pivoti = lefti\n",
    "        swap(l, pivoti, endi-1)\n",
    "        #recursive sort the first half of the array\n",
    "        quicksort_helper(l, starti, pivoti-1)\n",
    "        #recursive sort the second half of the array\n",
    "        quicksort_helper(l, pivoti+1, endi)\n",
    "        \n",
    "def quicksort(l):\n",
    "    quicksort_helper(l, 0, len(l)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform quicksort on data\n",
    "data = np.random.permutation(10)\n",
    "quicksort(data)\n",
    "assert np.all(data == np.array([0,1,2,3,4,5,6,7,8,9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform quicksort on datasets with different sizes and store timings\n",
    "timings_fast = apply_and_time_sorting(datasets, quicksort)\n",
    "print(timings_fast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now plot the timings for the naive and quicksort algorithm. What does this tell you about the complexity of the algorithms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=data_sizes,\n",
    "                y=timings_slow,\n",
    "                mode='lines',\n",
    "                name='slow'))\n",
    "fig.add_trace(go.Scatter(x=data_sizes,\n",
    "                y=timings_fast,\n",
    "                mode='lines',\n",
    "                name='fast'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The next algorithm we will investigate is the community detection algorithm: the Louvain method\n",
    "\n",
    "### As a simple example we will take the social network of a karate club collected by Zachary, the members have a link between them in case they interacted with each other outside the club. The goal is to find the communities that exist in the club\n",
    "\n",
    "First download the file from https://raw.githubusercontent.com/ddvlamin/data_engineering_ua/master/code_examples/week4/data/karate_edges_78.txt\n",
    "and put it in the same folder as the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"./karate_edges_78.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's first create the adjacency matrix for the social network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_adjacency_matrix(file):\n",
    "    node_neighbors = defaultdict(list)\n",
    "\n",
    "    with open(file, \"r\") as fin:\n",
    "        for line in fin:\n",
    "            node_1, node_2 = line.strip().split('\\t')\n",
    "            node_1 = int(node_1)-1 #offset to make the integers start from zero\n",
    "            node_2 = int(node_2)-1\n",
    "            node_neighbors[node_1].append(node_2)\n",
    "\n",
    "    number_of_nodes = len(node_neighbors)\n",
    "    A = np.zeros((number_of_nodes, number_of_nodes))\n",
    "\n",
    "    for node, neighbors in node_neighbors.items():\n",
    "        for neighbor in neighbors:\n",
    "            A[node, neighbor] = 1\n",
    "\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = create_adjacency_matrix(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the total sum of edges in the adjacency matrix A (divided by 2)\n",
    "Hint: use np.sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = CODE_HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's now define a function to compute the modularity of the network given the adjacency matrix A, the communities and the total sum of edge weights\n",
    "Hint: look at the modularity formula in the course slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_modularity(A, communities, m):\n",
    "    \"\"\"\n",
    "    computes the modularity for a given network A and communities communities (see formula in slides)\n",
    "    \n",
    "    A: dense square matrix, nonzero element on row i and column j indicating \n",
    "        a weighted connection between vertex i and j\n",
    "    communities: vector of communities, on position i the community for node i, so the length of the list equals \n",
    "        the number of nodes in the graph\n",
    "    m: an integer that equals the sum of all edges in the graph divided by two\n",
    "    \"\"\"\n",
    "    nrows, ncols = A.shape\n",
    "    assert nrows == ncols\n",
    "    nvertices = nrows\n",
    "\n",
    "    modularity = 0.0\n",
    "    #iterate over all pair of vertices\n",
    "    for vertexi in range(nvertices):\n",
    "        for vertexj in range(nvertices):\n",
    "            #sum all the edge weight of vertexi\n",
    "            ki = CODE_HERE\n",
    "            #sum all the edge weight of vertexj\n",
    "            kj = CODE_HERE\n",
    "            #if both vertices are in the same community add a term to the modularity\n",
    "            if communities[vertexi] == communities[vertexj]:\n",
    "                modularity += CODE_HERE\n",
    "\n",
    "    return modularity/(2*m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the modularity putting every vertex/node in its own community\n",
    "\n",
    "The community vector has the same length as the number of vertices/nodes in the graph <br>\n",
    "Position i in the community vector defines the community of vertex/node i <br>\n",
    "e.g. [0,0,1,1,1] means the first two vertices belong the community 0 and the last three to community 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_communities(n_vertices):\n",
    "    return CODE_HERE # use np.arange to create a vector from 0 to n_vertices-1\n",
    "    \n",
    "initial_communities = initialize_communities(A.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now compute the modularity for the given adjancency matrix, the initial communities and the total sum of edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_modularity = compute_modularity(A, initial_communities, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert initial_modularity == -0.04980276134122286"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we will implement the first phase of the Louvain method but using a naive, inefficient way of computing the modularity difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first define some helper function to perform the local search (i.e. the first phase) <br>\n",
    "\n",
    "The first helper function let's us compute the difference in modularity when we move a node from its current\n",
    "community into another community <br>\n",
    "\n",
    "We will (on purpose) compute this in a very naive inefficient way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modularity_difference(A, communities, m, modularity, vertex, community):\n",
    "    \"\"\"\n",
    "    A: the adjacency matrix of the graph\n",
    "    communities: vector of communities, on position i the community for node i, so the length of the list equals \n",
    "        the number of nodes in the graph \n",
    "    m: an integer that equals the sum of all edges in the graph divided by two\n",
    "    modularity: the modularity of the graph A with communities defined by the variable communities\n",
    "    vertex: the index of the vertex/node you want to compute the modularity difference for when moving it to community\n",
    "    community: the community to which you want move the vertex\n",
    "    \"\"\"\n",
    "    #store the vertex' community in a variable\n",
    "    original_community = CODE_HERE \n",
    "    #change community of the current vertex\n",
    "    communities[vertex] = CODE_HERE\n",
    "    #compute the new modularity use the compute_modularity function\n",
    "    new_modularity = CODE_HERE\n",
    "    #compute the difference between the new modulariy and the old one\n",
    "    modularity_difference = CODE_HERE\n",
    "    #restore original community of the vertex\n",
    "    communities[vertex] = CODE_HERE\n",
    "    return modularity_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_A = np.array([\n",
    "    [0,1,1,0,0,0],\n",
    "    [1,0,1,0,1,0],\n",
    "    [1,1,0,0,0,0],\n",
    "    [0,0,0,0,1,1],\n",
    "    [0,1,0,1,0,1],\n",
    "    [0,0,0,1,1,0]\n",
    "    ]\n",
    ")\n",
    "test_communities = np.arange(0,6)\n",
    "test_m = 0.5*np.sum(A)\n",
    "test_mod = compute_modularity(test_A, test_communities, test_m)\n",
    "expected = [0.0, 0.01232741617357002, 0.012491781722550954, -0.0003287310979618672, -0.0004930966469428009, -0.0003287310979618672]\n",
    "for vertexi in range(0,6):\n",
    "    assert expected[vertexi] == modularity_difference(test_A, test_communities, test_m, test_mod, 0, vertexi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now use the modularity_difference function to iterate over all communities and see which community for the given vertex gives the biggest increase in modularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_community(A, communities, m, modularity, vertex):\n",
    "    \"\"\"\n",
    "    A: the adjacency matrix of the graph\n",
    "    communities: vector of communities, on position i the community for node i, so the length of the list equals \n",
    "        the number of nodes in the graph \n",
    "    m: an integer that equals the sum of all edges in the graph divided by two\n",
    "    modularity: the modularity of the graph A with communities defined by the variable communities\n",
    "    vertex: the index of the vertex/node for which you want to find the best community\n",
    "    \"\"\"    \n",
    "    unique_communities = np.unique(communities)\n",
    "    max_modularity_difference = 0\n",
    "    best_community = communities[vertex]\n",
    "    for community in unique_communities:\n",
    "        #compute the modularity difference for the given vertex and the community by applying the \n",
    "        #before defined modularity_difference function\n",
    "        modularity_diff = CODE_HERE\n",
    "        #if the difference is larger than the maximum modularity difference, store the new difference\n",
    "        #and set the best_community variable to this new community\n",
    "        if modularity_diff > max_modularity_difference:\n",
    "            max_modularity_difference = CODE_HERE\n",
    "            best_community = CODE_HERE\n",
    "    return best_community, max_modularity_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_A = np.array([\n",
    "    [0,1,1,0,0,0],\n",
    "    [1,0,1,0,1,0],\n",
    "    [1,1,0,0,0,0],\n",
    "    [0,0,0,0,1,1],\n",
    "    [0,1,0,1,0,1],\n",
    "    [0,0,0,1,1,0]\n",
    "    ]\n",
    ")\n",
    "test_communities = np.arange(0,6)\n",
    "test_m = 0.5*np.sum(A)\n",
    "test_mod = compute_modularity(test_A, test_communities, test_m)\n",
    "assert find_best_community(test_A, test_communities, test_m, test_mod, 0)[0] == 2\n",
    "assert find_best_community(test_A, test_communities, test_m, test_mod, 4)[0] == 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will now use the find_best_community function to keep iterating over all nodes until we can't increase the modularity anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_communities(communities):\n",
    "    \"\"\"\n",
    "    reset the community vector so that the communities  start from zero\n",
    "    \n",
    "    Example:\n",
    "    --------\n",
    "    communities = [17,17,3,17,3,1,1,1,3,17]\n",
    "    reset_communities(communities) == [0,0,1,0,1,2,2,2,1,0]\n",
    "    \"\"\"\n",
    "    counter = count()\n",
    "    community_mapper = defaultdict(lambda: next(counter))\n",
    "    reset_communities = np.zeros((len(communities),), dtype=int)\n",
    "    for vertex, community in enumerate(communities):\n",
    "        reset_communities[vertex] = community_mapper[community]\n",
    "    return reset_communities\n",
    "\n",
    "def local_search(A, communities, m, modularity, seed=11):\n",
    "    #we iterate over the vertices in a randomized way\n",
    "    nvertices = len(communities)\n",
    "    if seed>0:\n",
    "        vertex_order = np.random.RandomState(seed=seed).permutation(nvertices)\n",
    "    else:\n",
    "        vertex_order = range(nvertices)\n",
    "\n",
    "    is_modularity_increasing = True\n",
    "    #keep iterating over the vertices until the modulariy stops increasing\n",
    "    while is_modularity_increasing:\n",
    "        is_modularity_increasing = False\n",
    "        for vertex in vertex_order:\n",
    "            #find the best community for the vertex using the find_best_community function\n",
    "            best_community, modularity_difference = CODE_HERE\n",
    "            #if the modulariy is strictly positive set the community of the vertex to the best_community\n",
    "            if modularity_difference > 0:\n",
    "                communities[vertex] = CODE_HERE\n",
    "                modularity += CODE_HERE #update the modularity with the difference\n",
    "                is_modularity_increasing = True #set the flag that indicates the modularity is still increasing\n",
    "\n",
    "    communities = reset_communities(communities) #just reindexing the new communities so they start from zero\n",
    "\n",
    "    return communities, modularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_A = np.array([\n",
    "    [0,1,1,0,0,0],\n",
    "    [1,0,1,0,1,0],\n",
    "    [1,1,0,0,0,0],\n",
    "    [0,0,0,0,1,1],\n",
    "    [0,1,0,1,0,1],\n",
    "    [0,0,0,1,1,0]\n",
    "    ]\n",
    ")\n",
    "test_communities = np.arange(0,6)\n",
    "test_m = 0.5*np.sum(A)\n",
    "test_mod = compute_modularity(test_A, test_communities, test_m)\n",
    "assert np.all(local_search(test_A, test_communities, test_m, test_mod)[0] == np.array([0,0,0,1,1,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next we implement the second phase of the Louvain method\n",
    "This step reduces the network where all nodes, that belong to the same community, are now reduced into one node <br>\n",
    "The new weight between two nodes is the sum of all the edge weights between their respective communities <br>\n",
    "The weight of the self-link is the sum of all weights between the original nodes of the community <br>\n",
    "\n",
    "Example <br>\n",
    "\n",
    "Original adjacency matrix <br>\n",
    "```python\n",
    "    [\n",
    "        [0,1,1,0],\n",
    "        [1,0,1,0],\n",
    "        [1,1,0,1],\n",
    "        [0,0,1,0]\n",
    "    ]\n",
    "```\n",
    "with communities <br>\n",
    "```python\n",
    "    [0,0,0,1]\n",
    "```\n",
    "will reduce to <br>\n",
    "```python\n",
    "    [\n",
    "        [6,1],\n",
    "        [1,0]\n",
    "    ]\n",
    "```\n",
    "with communities <br>\n",
    "```python\n",
    "    [0,1]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_network(A, communities):\n",
    "    #assumes communities starting from zero increasing by one\n",
    "    unique_communities = np.unique(communities)\n",
    "    n_communities = len(unique_communities)\n",
    "    #initialize the new reduced adjacency matrix as a zero square matrix of dimension n_communities\n",
    "    reduced_A = CODE_HERE\n",
    "\n",
    "    #now let's fill in all the entries in the reduced_A matrix \n",
    "    #(note that this can be done more efficiently because the matrix is symmetric) \n",
    "    for communityi in unique_communities:\n",
    "        for communityj in unique_communities:\n",
    "            #find all the vertex indices that belong to communityi \n",
    "            #(hint: use np.argwhere and don't forget to flatten the array)\n",
    "            vertices_i = CODE_HERE\n",
    "            #find all the vertex indices that belong to communityj \n",
    "            vertices_j = CODE_HERE\n",
    "            #select the sub matrix of all the vertices in communityi and communityj\n",
    "            sub_A = A[vertices_i,:]\n",
    "            sub_A = sub_A[:,vertices_j]\n",
    "            #the edge weight between communityi and communityj is the sum of the weights\n",
    "            #between all the vertices of communities communityi and communityj\n",
    "            reduced_A[communityi, communityj] = CODE_HERE\n",
    "\n",
    "    #put each vertex in the reduced network again in its own community\n",
    "    communities = np.array([i for i in range(reduced_A.shape[0])], dtype=int)\n",
    "    \n",
    "    return reduced_A, communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_A = np.array([\n",
    "    [0,1,1,0,0,0],\n",
    "    [1,0,1,0,1,0],\n",
    "    [1,1,0,0,0,0],\n",
    "    [0,0,0,0,1,1],\n",
    "    [0,1,0,1,0,1],\n",
    "    [0,0,0,1,1,0]\n",
    "    ]\n",
    ")\n",
    "test_communities = [0,0,0,1,1,1]\n",
    "assert np.all(reduce_network(test_A, test_communities)[0] == np.array([[6,1],[1,6]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we will combine the local_search function and the reduce_function to find the community structure with the highest modularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traceback_communities(reduced_communities, communities):\n",
    "    #assumes community i after local search is also reduced to the i-th node\n",
    "    new_communities = np.zeros((len(communities,)),dtype=int)\n",
    "    for vertex, community in enumerate(communities):\n",
    "        new_communities[vertex] = reduced_communities[community]\n",
    "    return new_communities\n",
    "\n",
    "def detect_communities(A, communities):\n",
    "    m = 0.5*np.sum(np.sum(A))\n",
    "    modularity = compute_modularity(A, communities, m)\n",
    "\n",
    "    #first do a local_search to find better community structure using the local_search function\n",
    "    local_communities, modularity = CODE_HERE\n",
    "\n",
    "    #as long as we are able to further combine communities, we keep optimizing\n",
    "    if len(np.unique(local_communities)) < len(communities):\n",
    "        #reduce the network with the communities found by the previous local search\n",
    "        reduced_A, reduced_communities = CODE_HERE\n",
    "        #keep repeating these two phases (local_search and reduce_network) \n",
    "        #by a recursive call to detect_communities using the reduced network reduced_A and\n",
    "        #the reduced communities reduced_communities\n",
    "        #if this recursion makes your head explode, that's normal, don't worry\n",
    "        reduced_communities = CODE_HERE\n",
    "        #a function to trace back the communities of the original nodes using the reduced network communities\n",
    "        expanded_communities = traceback_communities(reduced_communities, local_communities)\n",
    "        return expanded_communities\n",
    "    else:\n",
    "        return communities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's now apply the algorithm to the Karate Club network and compute the new modularity\n",
    "Let's also time it to see how slow it is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_communities = initialize_communities(A.shape[0])\n",
    "\n",
    "start_time = time.time()\n",
    "best_communities = detect_communities(A, initial_communities)\n",
    "end_time = time.time()\n",
    "time_slowest = end_time-start_time\n",
    "print(f\"Took {time_slowest} seconds to execute\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert compute_modularity(A, best_communities, m) > 0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's visualize the network with the optimal commmunities using the below functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(A, communities):\n",
    "    G=nx.Graph()\n",
    "    \n",
    "    for nodei in range(A.shape[0]):\n",
    "        d = {\"community\": communities[nodei]}\n",
    "        G.add_node(nodei+1, **d)\n",
    "    \n",
    "    for i, j in zip(*A.nonzero()):\n",
    "        if communities[i] == communities[j]:\n",
    "            weight = 100\n",
    "        else:\n",
    "            weight = 1\n",
    "        G.add_edge(i+1, j+1, weight=weight)\n",
    "        \n",
    "    return G\n",
    "\n",
    "def plot_graph(G, pos=None):\n",
    "    plt.figure(figsize=(30,15))  \n",
    "    \n",
    "    node_communities = [d[\"community\"] for _, d in G.nodes(data=True)]\n",
    "    \n",
    "    if pos==None:\n",
    "        pos = nx.spring_layout(G, scale=30)  # positions for all nodes\n",
    "    \n",
    "    nx.draw_networkx_nodes(G, pos, node_color=node_communities)\n",
    "    nx.draw_networkx_edges(G, pos)\n",
    "\n",
    "    nx.draw_networkx_labels(G, pos, font_size=30, font_family='sans-serif')\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    return pos\n",
    "\n",
    "G = create_graph(A, best_communities)\n",
    "plot_graph(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's now replace the naive computation of the modularity difference with the difference formula presented in the paper\n",
    "\n",
    "Link to paper: https://www.researchgate.net/publication/1913681_Fast_Unfolding_of_Communities_in_Large_Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modularity_difference(A, communities, m, modularity, vertex, community):\n",
    "    \"\"\"\n",
    "    A: the adjacency matrix of the graph\n",
    "    communities: vector of communities, on position i the community for node i, so the length of the list equals \n",
    "        the number of nodes in the graph \n",
    "    m: an integer that equals the sum of all edges in the graph divided by two\n",
    "    modularity: the modularity of the graph A with communities defined by the variable communities\n",
    "    vertex: the index of the vertex/node you want to compute the modularity difference for when moving it to community\n",
    "    community: the community to which you want move the vertex\n",
    "    \"\"\"\n",
    "    #store the vertex' community in a variable\n",
    "    old_community = communities[vertex] \n",
    "    \n",
    "    #True for all the nodes that belong to the new community to which we want to move our current vertex\n",
    "    new_community_nodes = communities == community\n",
    "    new_community_nodes[vertex] = False\n",
    "    #True for all the nodes that belong to the old community of the vertex\n",
    "    old_community_nodes = communities == old_community\n",
    "    old_community_nodes[vertex] = False\n",
    "    \n",
    "    new_k_i_in = CODE_HERE #sum all the weights of edges going from the vertex to nodes of its new community\n",
    "    old_k_i_in = CODE_HERE #sum all the weights of edges going from the vertex to nodes of its old community\n",
    "    \n",
    "    ki = np.sum(A[vertex,:])\n",
    "    \n",
    "    new_sum_tot = CODE_HERE #sum all weights of edges incident to nodes of the new community\n",
    "    old_sum_tot = CODE_HERE #sum all weights of edges incident to nodes of the old community\n",
    "    \n",
    "    #formula for the fast computation of the modularity difference\n",
    "    modularity_difference = (1/m)*(new_k_i_in-old_k_i_in) + (1/(2*m*m))*ki*(old_sum_tot-new_sum_tot)\n",
    "    \n",
    "    return modularity_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_A = np.array([\n",
    "    [0,1,1,0,0,0],\n",
    "    [1,0,1,0,1,0],\n",
    "    [1,1,0,0,0,0],\n",
    "    [0,0,0,0,1,1],\n",
    "    [0,1,0,1,0,1],\n",
    "    [0,0,0,1,1,0]\n",
    "    ]\n",
    ")\n",
    "test_communities = np.arange(0,6)\n",
    "test_m = 0.5*np.sum(A)\n",
    "test_mod = compute_modularity(test_A, test_communities, test_m)\n",
    "expected = [\n",
    "    (0.0,0.0), \n",
    "    (0.012327416173,0.012327416174), \n",
    "    (0.012491781722,0.012491781723), \n",
    "    (-0.00032873110,-0.00032873109), \n",
    "    (-0.00049309665,-0.00049309664), \n",
    "    (-0.00032873110,-0.00032873109)\n",
    "]\n",
    "for vertexi in range(0,6):\n",
    "    mod_diff = modularity_difference(test_A, test_communities, test_m, test_mod, 0, vertexi)\n",
    "    lb, ub = expected[vertexi]\n",
    "    assert lb <= mod_diff <= ub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_communities = initialize_communities(A.shape[0])\n",
    "start_time = time.time()\n",
    "best_communities = detect_communities(A, initial_communities)\n",
    "end_time = time.time()\n",
    "time_faster = end_time-start_time\n",
    "print(f\"Took {time_faster} seconds to execute\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert compute_modularity(A, best_communities, m) > 0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will now compare this naive implementation with a fast implementation of the Louvain method:\n",
    "https://python-louvain.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install the library\n",
    "!pip install python-louvain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "import community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert adjacency matrix to graph\n",
    "G = nx.from_numpy_matrix(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the documentation to find out which command to use to compute the partition using the fast louvain library. Compare execution time to the naive implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the best partition\n",
    "start_time = time.time()\n",
    "partition = CODE_HERE\n",
    "time_fastest = time.time()-start_time\n",
    "print(f\"Took {time_fastest} seconds to execute\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the network using the previously defined functions and compare this to the previous solution. What do you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize network\n",
    "CODE_HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the computation times in a bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(data=[go.Bar(\n",
    "            x=[\"slow\",\"faster\",\"fastest\"], y=[time_slowest, time_faster, time_fastest],\n",
    "            text=[time_slowest, time_faster, time_fastest],\n",
    "            textposition='auto',\n",
    "        )])\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
