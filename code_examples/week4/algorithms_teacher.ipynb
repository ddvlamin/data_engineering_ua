{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this lab session we will implement two algorithms (an algorithm for sorting and one for detecting communities) each in two different ways: a naive, less efficient way and a better, faster way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from itertools import count\n",
    "\n",
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.offline as py\n",
    "from plotly.offline import init_notebook_mode\n",
    "import plotly.graph_objs as go\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's start with the sorting algorithm\n",
    "We will create multiple data sets of integers we want to sort <br>\n",
    "Each data set bigger than the previous one <br>\n",
    "These different sized data sets will be use to emperically determine the complexity of the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_and_time_sorting(datasets, sort_function):\n",
    "    timings = []\n",
    "    for dataset in datasets:\n",
    "        start = time.time()\n",
    "        sort_function(dataset)\n",
    "        end = time.time()\n",
    "        time_difference = end - start\n",
    "        timings.append(time_difference)\n",
    "    return timings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = []\n",
    "for data_size in [5000,10000,20000]:\n",
    "    datasets.append(np.random.permutation(data_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we will implement the naive insertion sorting algorithm \n",
    "It's an algorithm that sorts an array in-place by iterating over the elements and inserting the current element\n",
    "in the part of the array that is already sorted\n",
    "\n",
    "```python\n",
    "[1,11,13,7,4,8,0]\n",
    "```\n",
    "\n",
    "If the current element is 7, then the part of the algorithm before 7 is already sorted by the algorithm <br>\n",
    "It then tries to find the position to insert the element 7 in the part of the array that is already sorted <br>\n",
    "Resulting in\n",
    "\n",
    "```python\n",
    "[1,7,11,13,4,8,0]\n",
    "```\n",
    "\n",
    "The next element to be sorted is 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naivesort(data):\n",
    "    for i, element in enumerate(data):\n",
    "        j = i\n",
    "        while j>0 and element<data[j-1]: #iterate back into the array as long as the current element is smaller\n",
    "            data[j] = data[j-1] #put the bigger element in the position of the current one\n",
    "            j -= 1\n",
    "        data[j] = element # store the current element in its sorted position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.3475136756896973, 13.507608413696289, 54.321414947509766]\n"
     ]
    }
   ],
   "source": [
    "timings = apply_and_time_sorting(datasets, naivesort)\n",
    "print(timings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's compare this with the quicksort algorithm that was discussed in class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = []\n",
    "for data_size in [5000,10000,20000]:\n",
    "    datasets.append(np.random.permutation(data_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap(l, i, j):\n",
    "    tmp = l[j]\n",
    "    l[j] = l[i]\n",
    "    l[i] = tmp\n",
    "\n",
    "def quicksort_helper(l, starti, endi):\n",
    "    #set_trace()\n",
    "    if starti >= endi:\n",
    "        return\n",
    "    if endi-starti+1<=100:\n",
    "        naivesort(l)\n",
    "    else:\n",
    "        middlei = int(0.5*(starti+endi))\n",
    "        #first make sure the start, middle and end element are in sorted order\n",
    "        if (l[middlei]<l[starti]):\n",
    "            swap(l,starti, middlei)\n",
    "        if (l[endi]<l[starti]):\n",
    "            swap(l, starti, endi)\n",
    "        if (l[endi]<l[middlei]):\n",
    "            swap(l, middlei, endi)\n",
    "        pivoti = middlei\n",
    "        \n",
    "        #put the pivot element just before the last \n",
    "        swap(l, pivoti, endi-1)\n",
    "        pivot = l[endi-1]\n",
    "        lefti = starti+1\n",
    "        righti = endi-2\n",
    "        \n",
    "        #now make sure the elements are partially sorted,\n",
    "        #i.e. elements in the first half should be smaller than the pivot element\n",
    "        #the elements in the second half should be larger than the pivot element\n",
    "        while True:\n",
    "            while l[lefti]<pivot: #move the cursor from left to right until you encounter an element larger than the pivot\n",
    "                lefti += 1\n",
    "            while pivot<l[righti]: #move the cursor from right to left until you encounter an element larger than the pivot\n",
    "                righti -= 1\n",
    "            if lefti < righti: #the right element should be smaller than the left element, swap them \n",
    "                swap(l, lefti, righti)\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "        pivoti = lefti\n",
    "        swap(l, pivoti, endi-1)\n",
    "        #recursive sort the first half of the array\n",
    "        quicksort_helper(l, starti, pivoti-1)\n",
    "        #recursive sort the second half of the array\n",
    "        quicksort_helper(l, pivoti+1, endi)\n",
    "        \n",
    "def quicksort(l):\n",
    "    quicksort_helper(l, 0, len(l)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.248666524887085, 12.91700005531311, 19.3950617313385]\n"
     ]
    }
   ],
   "source": [
    "timings = apply_and_time_sorting(datasets, quicksort)\n",
    "print(timings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The next algorithm we will investigate is the community detection algorithm: the Louvain method\n",
    "\n",
    "### As a simple example we will take the social network of a karate club collected by Zachary, the members have a link between them in case they interacted with each other outside the club. The goal is to find the communities that exist in the club"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"./data/karate_edges_78.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's first create the adjacency matrix for the social network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_adjacency_matrix(file):\n",
    "    node_neighbors = defaultdict(list)\n",
    "\n",
    "    with open(file, \"r\") as fin:\n",
    "        for line in fin:\n",
    "            node_1, node_2 = line.strip().split('\\t')\n",
    "            node_1 = int(node_1)-1 #offset to make the integers start from zero\n",
    "            node_2 = int(node_2)-1\n",
    "            node_neighbors[node_1].append(node_2)\n",
    "\n",
    "    number_of_nodes = len(node_neighbors)\n",
    "    A = np.zeros((number_of_nodes, number_of_nodes))\n",
    "\n",
    "    for node, neighbors in node_neighbors.items():\n",
    "        for neighbor in neighbors:\n",
    "            A[node, neighbor] = 1\n",
    "\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = create_adjacency_matrix(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the total sum of edges in the adjacency matrix A (divided by 2)\n",
    "Hint: use np.sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 0.5*np.sum(np.sum(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's now define a function to compute the modularity of the network given the adjacency matrix A, the communities and the total sum of edge weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_modularity(A, communities, m):\n",
    "    \"\"\"\n",
    "    A: dense square matrix, nonzero element on row i and column j indicating \n",
    "        a weighted connection between vertex i and j\n",
    "    communities: vector of communities, on position i the community for node i, so the length of the list equals \n",
    "        the number of nodes in the graph\n",
    "    m: an integer that equals the sum of all edges in the graph divided by two\n",
    "    \"\"\"\n",
    "    nrows, ncols = A.shape\n",
    "    assert nrows == ncols\n",
    "    nvertices = nrows\n",
    "\n",
    "    modularity = 0.0\n",
    "    for vertexi in range(nvertices):\n",
    "        for vertexj in range(nvertices):\n",
    "            ki = np.sum(A[vertexi,:])\n",
    "            kj = np.sum(A[vertexj,:])\n",
    "            if communities[vertexi] == communities[vertexj]:\n",
    "                modularity += A[vertexi, vertexj] - (ki*kj)/(2*m)\n",
    "\n",
    "    return modularity/(2*m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the modularity putting every vertex/node in its own community\n",
    "\n",
    "The community vector has the same length as the number of vertices/nodes in the graph <br>\n",
    "Position i in the community vector defines the community of vertex/node i <br>\n",
    "e.g. [0,0,1,1,1] means the first two vertices belong the community 0 and the last three to community 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_communities(n_vertices):\n",
    "    return np.arange(0,n_vertices) # use np.arange to create a vector from 0 to n_vertices-1\n",
    "    \n",
    "communities = initialize_communities(A.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now compute the modularity for the given adjancency matrix, the initial communities and the total sum of edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_modularity = compute_modularity(A, communities, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert initial_modularity == -0.04980276134122286"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we will implement the first phase of the Louvain method but using a naive, inefficient way of computing the modularity difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first define some helper function to perform the local search (i.e. the first phase) <br>\n",
    "\n",
    "The first helper function let's us compute the difference in modularity when we move a node from its current\n",
    "community into another community <br>\n",
    "\n",
    "We will (on purpose) compute this in a very naive inefficient way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modularity_difference(A, communities, m, modularity, vertex, community):\n",
    "    \"\"\"\n",
    "    A: the adjacency matrix of the graph\n",
    "    communities: vector of communities, on position i the community for node i, so the length of the list equals \n",
    "        the number of nodes in the graph \n",
    "    m: an integer that equals the sum of all edges in the graph divided by two\n",
    "    modularity: the modularity of the graph A with communities defined by the variable communities\n",
    "    vertex: the index of the vertex/node you want to compute the modularity difference for when moving it to community\n",
    "    community: the community to which you want move the vertex\n",
    "    \"\"\"\n",
    "    #save community of node\n",
    "    original_community = communities[vertex]\n",
    "    #change community\n",
    "    communities[vertex] = community\n",
    "    #compute new modularity\n",
    "    new_modularity = compute_modularity(A, communities, m)\n",
    "    #print(f\"{nodei}, {communityi}, {self.modularity}, {new_modularity}\")\n",
    "    modularity_difference = new_modularity-modularity\n",
    "    #restore community\n",
    "    communities[vertex] = original_community\n",
    "    return modularity_difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now use the modularity_difference function to iterate over all communities and see which community for the given vertex gives the biggest increase in modularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_community(A, communities, m, modularity, vertex):\n",
    "    \"\"\"\n",
    "    A: the adjacency matrix of the graph\n",
    "    communities: vector of communities, on position i the community for node i, so the length of the list equals \n",
    "        the number of nodes in the graph \n",
    "    m: an integer that equals the sum of all edges in the graph divided by two\n",
    "    modularity: the modularity of the graph A with communities defined by the variable communities\n",
    "    vertex: the index of the vertex/node for which you want to find the best community\n",
    "    \"\"\"    \n",
    "    unique_communities = np.unique(communities)\n",
    "    max_modularity_difference = 0\n",
    "    best_community = communities[vertex]\n",
    "    for community in unique_communities:\n",
    "        modularity_diff = modularity_difference(A, communities, m, modularity, vertex, community)\n",
    "        print(f\"modularity difference when moving {vertex} to community {community}: {modularity_diff}\")\n",
    "        if modularity_diff > max_modularity_difference:\n",
    "            max_modularity_difference = modularity_diff\n",
    "            best_community = community\n",
    "    return best_community, max_modularity_difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will now use the find_best_community function to keep iterating over all nodes until we can't increase the modularity anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_search(A, communities, m, modularity, seed=11):\n",
    "    nvertices = len(communities)\n",
    "    if seed>0:\n",
    "        vertex_order = np.random.RandomState(seed=seed).permutation(nvertices)\n",
    "    else:\n",
    "        vertex_order = range(nvertices)\n",
    "\n",
    "    is_modularity_increasing = True\n",
    "    while is_modularity_increasing:\n",
    "        is_modularity_increasing = False\n",
    "        for vertex in vertex_order:\n",
    "            best_community, modularity_difference = find_best_community(A, communities, m, modularity, vertex)\n",
    "            if modularity_difference > 0:\n",
    "                communities[vertex] = best_community\n",
    "                modularity += modularity_difference\n",
    "                is_modularity_increasing = True\n",
    "\n",
    "    communities = reset_communities(communities)\n",
    "\n",
    "    print(f\"Finished phase1, new modularity: {modularity}\")\n",
    "\n",
    "    return communities, modularity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
