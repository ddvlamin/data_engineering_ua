{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labsession Pyspark\n",
    "In this labsession we will be covering pyspark, the pythong implementation of Apache Spark, which uses the MapReduce paradigm. As was the case for previous sessions, fill in the CODE_HERE placeholders. Check your work by running the include ASSERT-statements.\n",
    "\n",
    "The documentation for Pyspark is located at https://spark.apache.org/docs/latest/api/python/pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, the necesary packages are initialised\n",
    "You do not need to code the following three blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "#REMARK: if you installed it yourself on Windows (following the tutorial) you might need to uncomment the next two lines\n",
    "import findspark\n",
    "findspark.init()\n",
    "import numpy as np\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "conf = pyspark.SparkConf().setAppName('appName').setMaster('local')\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise 1: Wordcount\n",
    "For the first excercise we will be performing wordcount on a sample file.\n",
    "\n",
    "First, the textfile, located in \"data/wordcount.txt\" needs to be converted to an RDD, and then split up.\n",
    "Hint: use sc.textFile and flatMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = CODE_HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, count the actual word. Hint: Use map & reduceByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCounts=CODE_HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, save the output in a human readable format. You can check the output of the wordcount by going to \"data/output/part-00000\". Note, if you re-run the following black, you wil get errors because the output files already exist. If you want to re-run this part of the code, delete the output files first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCounts.saveAsTextFile(\"data/output/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise 2: Ecommerce data\n",
    "In this integrated excercise, we will be processing ecommerce data\n",
    "### Let's load the ecommerce data and transform it into an RDD\n",
    "#### the file contains JSON objects of the form\n",
    "\n",
    "```\n",
    "{\n",
    "  \"\": \"9\",\n",
    "  \"Clothing ID\": \"1077\",\n",
    "  \"Age\": \"34\",\n",
    "  \"Title\": \"Such a fun dress!\",\n",
    "  \"Review Text\": \"I'm 5\\\"5' and 125 lbs. i ordered the s petite to make sure the length wasn't too long. i\n",
    " typically wear an xs regular in retailer dresses. if you're less busty (34b cup or smaller), a s petite w\n",
    "ill fit you perfectly (snug, but not tight). i love that i could dress it up for a party, or down for work\n",
    ". i love that the tulle is longer then the fabric underneath.\",\n",
    "  \"Rating\": \"5\",\n",
    "  \"Recommended IND\": \"1\",\n",
    "  \"Positive Feedback Count\": \"0\",\n",
    "  \"Division Name\": \"General\",\n",
    "  \"Department Name\": \"Dresses\",\n",
    "  \"Class Name\": \"Dresses\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecommerce_data = sc.textFile(\"./data/ecommerce.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First convert each line (which is a string) to JSON using json.loads\n",
    "\n",
    "#### remember: Spark uses lazy evaluation so applying map on an RDD does not apply the function on the data until you call a fucntion like collect(), take(), reduce(), count(), saveAsTextFile()... which triggers the execution\n",
    "\n",
    "Hint: use json.loads to transform a string into a JSON object <br>\n",
    "Hint: also put the RDD in memory using persist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecommerce_json = ecommerce_data.map(CODE_HERE).CODE_HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's first try to determine the average age of the reviewers on this e-commerce website\n",
    "\n",
    "#### First construct an RDD where we extract the Age field of each JSON record and transform it to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_rdd = ecommerce_json.map(lambda js: CODE_HERE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's make a sum of all ages so that we can later devide by the total number of records to get the average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_sum = age_rdd.reduce(lambda x, y: CODE_HERE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we still need to figure out how many records are in the RDD, use the count() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_records = age_rdd.CODE_HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### compute the average now using the sum and the number of records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_age = CODE_HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.isclose(avg_age,43.1985438)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will now do the same, but compute the average number of words in the Review Text\n",
    "#### you can simply use the split function with the space character to split the text into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nwords_sum = ecommerce_json.map(lambda js: len(CODE_HERE)).reduce(CODE_HERE)\n",
    "avg_nwords = CODE_HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.isclose(avg_nwords, 58.0843907)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next we will count the number of reviews per rating (i.e. 1, 2, 3, 4 and 5)\n",
    "\n",
    "#### Similar to the word count exercise, build a RDD of tuples with the rating and a count of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_counts = ecommerce_json.map(CODE_HERE).CODE_HERE.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sort the resulting tuples (after collecting the results) from high to low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_rating_counts = sorted(rating_counts, key = CODE_HERE, reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sorted_rating_counts[0] == ('5', 13131)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A second way to achieve the same thing is by first grouping all records together with the same key and then counting how many appear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_counts = ecommerce_json.map(lambda js: (CODE_HERE, None)).CODE_HERE.map(CODE_HERE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_rating_counts = sorted(CODE_HERE.collect(), CODE_HERE, reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sorted_rating_counts[0] == ('5', 13131)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next we will count the number of reviews per Department Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_counts = ecommerce_json.map(CODE_HERE).CODE_HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### let's filter out all the Department names with less than 1000 reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_counts_filtered = category_counts.filter(CODE_HERE).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(category_counts_filtered) == 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### select the Department with most reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "largest_category = sorted(CODE_HERE)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert largest_category == \"Tops\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### slightly more complex, let's now count the number of reviews per Department Name and Rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_rating_counts = (ecommerce_json\n",
    "                          .map(lambda js: (CODE_HERE,1))\n",
    "                          .CODE_HERE\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### only keep the counts for the Jackets department and sort from high to low number of reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jackets = category_rating_counts.CODE_HERE.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_jackets = CODE_HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sorted_jackets[0] == ((\"Jackets\",'5'), 631)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create per Clothing ID a list of all the ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2agelist = (ecommerce_json\n",
    "              .CODE_HERE #map\n",
    "              .groupByKey()\n",
    "              .CODE_HERE #map\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### keep only the Clothing IDs that have more than 500 reviews and compute both the average and standard deviation of the age per Clothing ID\n",
    "\n",
    "#### The output should be tuples of the following form\n",
    "\n",
    "```\n",
    "('Clothing ID', {\"avg\": 43.4564, \"std\": 12.14566})\n",
    "```\n",
    "\n",
    "Hint: use np.array and np.std and np.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_summary_statistics(tpl):\n",
    "    id, age_list = tpl\n",
    "    age_array = np.array(age_list)\n",
    "    return (id, {\"avg\": CODE_HERE, \"std\": CODE_HERE })\n",
    "\n",
    "stats = id2agelist.filter(CODE_HERE).map(CODE_HERE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sort the results according the average age from high to low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_stats = CODE_HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sorted_stats[0] == ('829', {'avg': 44.64136622390892, 'std': 12.447020037376479})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next we will compute the term frequency - inverse document frequency of all the words in the reviews\n",
    "\n",
    "https://en.wikipedia.org/wiki/Tf%E2%80%93idf <br>\n",
    "\n",
    "#### First we will compute the document frequencies, i.e. count how many times each word occurs in a review (use a regular expression for this to filter out real words (no punctuation and numbers)\n",
    "\n",
    "The output of the document_frequencies RDD should be tuples of the form\n",
    "\n",
    "```\n",
    "('word', document_count)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the regular expression to extract only words with at least one character\n",
    "regex = CODE_HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_terms(js):\n",
    "    words = set(regex.findall(CODE_HERE))\n",
    "    for word in words:\n",
    "        yield CODE_HERE #also lower case the words\n",
    "        \n",
    "#use flatMap and reduceByKey\n",
    "document_frequencies = ecommerce_json.CODE_HERE.CODE_HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert document_frequencies.filter(lambda x: x[0]==\"wonderful\").collect()[0][1] == 290"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we will compute the inverse document frequency using the formula np.log(number of documents/document count)\n",
    "\n",
    "Hint: we compute the number of document before and stored it in the variable n_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_document_frequencies = document_frequencies.CODE_HERE #map\n",
    "print(inverse_document_frequencies.take(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sorted(inverse_document_frequencies.collect(), key = lambda x: x[1], reverse=True)[0][0] == \"narrowing\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will now compute the term frequencies per document\n",
    "\n",
    "#### The output should be an RDD of tuples of the form\n",
    "\n",
    "The document_id can be found under the empty key, i.e. js[\"\"]\n",
    "\n",
    "```\n",
    "('word', ('document_id', count))\n",
    "```\n",
    "\n",
    "word has to be the first element in the tuple as we will use this as a key to join on at some point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_term_frequencies(js):\n",
    "    record_id = js[\"\"]\n",
    "    words = CODE_HERE #use regex to extract all words\n",
    "    word_count = Counter()\n",
    "    CODE_HERE #for loop to count record_id-word pairs in document, lower case the words\n",
    "    return [CODE_HERE for ((record_id, word), cnt) in word_count.items()]\n",
    "\n",
    "term_frequencies = CODE_HERE #use flatMap\n",
    "print(term_frequencies.take(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert term_frequencies.filter(lambda x: x[0]==\"it\" and x[1][0]=='1').collect()[0][1][1] == 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we need to join the inverse document frequencies with the term frequencies per document\n",
    "\n",
    "Hint: term frequencies should be stored in the term_frequrencies RDD and inverse document frequencies in the inverse_document_frequencies RDD\n",
    "\n",
    "#### the output of the join will be an RDD with tuples of the form\n",
    "\n",
    "```\n",
    "('word', (idf, ('document_id', word count/term frequency) ) )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_joined = CODE_HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tf_idf_joined.filter(lambda x: x[0]=='comfortable' and x[1][1][0]==\"0\").collect()[0] == ('comfortable', (2.073244314833701, ('0', 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now lets multiply the term frequency/word count of each document with the inverse document frequency of the word\n",
    "\n",
    "#### and now make the keys (i.e. first element of the tuples) of the records in the resulting RDD the id of the document so that we can group all words that belong to the same document together again\n",
    "\n",
    "The output of this RDD should look like:\n",
    "\n",
    "```\n",
    "('document_id', ('word', tf*idf))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = tf_idf_joined.map(lambda x: CODE_HERE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.isclose(tf_idf.filter(lambda x: x[0]=='0' and x[1][0]==\"comfortable\").collect()[0][1][1], 2.07324431)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally we need to group all words that belong to the same document\n",
    "\n",
    "#### The output should be a tuple of the form ('document_id', words_dict) where words_dict is a dict of the following format\n",
    "\n",
    "```\n",
    "{\n",
    "    \"word\": tf_idf_word\n",
    "    \"word2\": tf_idf_word2\n",
    "    ...\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_tf_idf(tpl):\n",
    "    document_id, tpls = tpl\n",
    "    words = CODE_HERE #build a dict mapping word to tf_idf\n",
    "    CODE_HERE #return tuple with document_id and words dictionary\n",
    "\n",
    "per_review = CODE_HERE #use groupByKey and map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected = {'comfortable': 2.073244314833701, 'silky': 5.235846040622725, 'wonderful': 4.394278854944507, 'absolutely': 3.3820511804752176, 'sexy': 4.717052247207557, 'and': 0.36522226581530454}\n",
    "assert per_review.filter(lambda x: x[0] == '0').collect()[0][1] == expected"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
